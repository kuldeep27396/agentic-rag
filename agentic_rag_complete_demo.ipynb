{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Agentic RAG: Complete Demo & Tutorial\n",
    "\n",
    "## Building an Intelligent RAG Pipeline with Local Documents and Web Search\n",
    "\n",
    "Welcome to this comprehensive demonstration of **Agentic RAG** - a powerful approach that combines the decision-making capabilities of AI agents with the adaptability of Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "### What You'll Learn:\n",
    "- üîç How to build a smart routing system that decides between local and web search\n",
    "- üìö Setting up vector databases for local document retrieval\n",
    "- üåê Creating web search and scraping agents for external information\n",
    "- üß† Implementing document relevance grading and query optimization\n",
    "- üîÑ Assembling everything into a cohesive, intelligent pipeline\n",
    "\n",
    "### Architecture Overview:\n",
    "```\n",
    "User Query ‚Üí Router ‚Üí Local DB Search OR Web Search ‚Üí Context Assembly ‚Üí Final Answer\n",
    "```\n",
    "\n",
    "This system intelligently determines the best source of information for each query and provides comprehensive, contextually relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Section 1: Environment Setup and API Configuration\n",
    "\n",
    "Before we start building our Agentic RAG system, we need to install the required packages and configure our API keys.\n",
    "\n",
    "### Required API Keys:\n",
    "- **Groq API**: For fast LLM responses\n",
    "- **Gemini API**: For web scraping agents  \n",
    "- **Serper.dev API**: For web search functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain-groq faiss-cpu crewai crewai_tools serper pypdf2 python-dotenv setuptools sentence-transformers huggingface_hub langchain langchain-community langchain-text-splitters langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import warnings\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Function to set API keys securely\n",
    "def set_api_key(key_name: str, description: str):\n",
    "    \"\"\"Set API key either from environment or user input\"\"\"\n",
    "    if key_name not in os.environ:\n",
    "        os.environ[key_name] = getpass.getpass(f\"Enter your {description}: \")\n",
    "    return os.environ[key_name]\n",
    "\n",
    "# Set up API keys\n",
    "print(\"üîë Setting up API keys...\")\n",
    "GROQ_API_KEY = set_api_key(\"GROQ_API_KEY\", \"Groq API Key\")\n",
    "GEMINI_API_KEY = set_api_key(\"GEMINI_API_KEY\", \"Gemini API Key\") \n",
    "SERPER_API_KEY = set_api_key(\"SERPER_API_KEY\", \"Serper.dev API Key\")\n",
    "\n",
    "print(\"‚úÖ API keys configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for Agentic RAG\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool\n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "from pydantic import BaseModel, Field\n",
    "import time\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")\n",
    "print(\"üöÄ Ready to build our Agentic RAG system!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Section 2: Initialize Language Models\n",
    "\n",
    "We'll set up two different language models:\n",
    "- **ChatGroq (Llama)**: For general tasks like routing and answer generation\n",
    "- **Gemini LLM**: For web scraping agents with higher temperature for creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Language Models\n",
    "print(\"ü§ñ Initializing Language Models...\")\n",
    "\n",
    "# Main LLM for routing and general tasks (Groq - Fast responses)\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-specdec\",\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "# LLM for web scraping agents (Gemini - More creative)\n",
    "crew_llm = LLM(\n",
    "    model=\"gemini/gemini-1.5-flash\",\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Language models initialized!\")\n",
    "print(f\"   - Main LLM: {llm.model}\")\n",
    "print(f\"   - Crew LLM: {crew_llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Section 3: Load and Process Local Documents\n",
    "\n",
    "We'll create a sample document and process it for our vector database. In a real scenario, you would load your own PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for demonstration\n",
    "# In practice, you would load your PDF files using PyPDFLoader\n",
    "\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Agentic RAG: Advanced Retrieval-Augmented Generation\n",
    "        \n",
    "        Agentic RAG represents a significant evolution in AI systems, combining the decision-making \n",
    "        capabilities of AI agents with traditional RAG approaches. Unlike standard RAG systems that \n",
    "        simply retrieve and generate, Agentic RAG systems can:\n",
    "        \n",
    "        1. Make intelligent routing decisions\n",
    "        2. Evaluate retrieval quality\n",
    "        3. Rewrite queries for better results\n",
    "        4. Combine multiple information sources\n",
    "        \n",
    "        Key Components:\n",
    "        - Decision Router: Determines information source\n",
    "        - Document Grader: Evaluates relevance\n",
    "        - Query Rewriter: Optimizes search terms\n",
    "        - Multi-source Retrieval: Local and web sources\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"agentic_rag_guide.pdf\", \"page\": 1}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Machine Learning Fundamentals\n",
    "        \n",
    "        Machine learning is a subset of artificial intelligence that enables computers to learn\n",
    "        and improve from experience without being explicitly programmed. There are three main\n",
    "        types of machine learning:\n",
    "        \n",
    "        1. Supervised Learning: Uses labeled training data\n",
    "        2. Unsupervised Learning: Finds patterns in unlabeled data  \n",
    "        3. Reinforcement Learning: Learns through interaction and rewards\n",
    "        \n",
    "        Popular algorithms include:\n",
    "        - Linear Regression\n",
    "        - Decision Trees\n",
    "        - Neural Networks\n",
    "        - Support Vector Machines\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"ml_basics.pdf\", \"page\": 1}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Vector Databases and Embeddings\n",
    "        \n",
    "        Vector databases are specialized databases designed to store and query high-dimensional\n",
    "        vectors efficiently. They are crucial for:\n",
    "        \n",
    "        - Semantic search capabilities\n",
    "        - Similarity matching\n",
    "        - Recommendation systems\n",
    "        - RAG implementations\n",
    "        \n",
    "        Popular vector databases include:\n",
    "        - FAISS (Facebook AI Similarity Search)\n",
    "        - Pinecone\n",
    "        - Weaviate\n",
    "        - Chroma\n",
    "        \n",
    "        Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"vector_db_guide.pdf\", \"page\": 1}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üìÑ Sample documents created!\")\n",
    "print(f\"   - Total documents: {len(sample_documents)}\")\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(f\"   - Document {i+1}: {doc['metadata']['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents into chunks\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"‚öôÔ∏è Processing documents into chunks...\")\n",
    "\n",
    "# Convert sample documents to LangChain Document objects\n",
    "documents = []\n",
    "for doc_data in sample_documents:\n",
    "    doc = Document(\n",
    "        page_content=doc_data[\"content\"].strip(),\n",
    "        metadata=doc_data[\"metadata\"]\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Size of each chunk\n",
    "    chunk_overlap=50,       # Overlap between chunks\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs first, then sentences\n",
    ")\n",
    "\n",
    "# Split all documents\n",
    "document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Documents processed!\")\n",
    "print(f\"   - Original documents: {len(documents)}\")\n",
    "print(f\"   - Document chunks: {len(document_chunks)}\")\n",
    "print(f\"   - Average chunk size: {sum(len(chunk.page_content) for chunk in document_chunks) // len(document_chunks)} characters\")\n",
    "\n",
    "# Show a sample chunk\n",
    "print(f\"\\nüìù Sample chunk:\")\n",
    "print(f\"Content: {document_chunks[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {document_chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Section 4: Create Vector Database for Document Retrieval\n",
    "\n",
    "Now we'll create a FAISS vector database using HuggingFace embeddings to enable semantic search over our document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Database\n",
    "print(\"üîç Creating vector database...\")\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Create FAISS vector database from documents\n",
    "vector_db = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "print(\"‚úÖ Vector database created successfully!\")\n",
    "print(f\"   - Embedding model: sentence-transformers/all-mpnet-base-v2\")\n",
    "print(f\"   - Total vectors: {vector_db.index.ntotal}\")\n",
    "\n",
    "# Function to retrieve relevant content from local database\n",
    "def get_local_content(query: str, k: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant content from the vector database\"\"\"\n",
    "    try:\n",
    "        docs = vector_db.similarity_search(query, k=k)\n",
    "        content = \" \".join([doc.page_content for doc in docs])\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving local content: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is Agentic RAG?\"\n",
    "test_result = get_local_content(test_query, k=2)\n",
    "print(f\"\\nüß™ Test retrieval for '{test_query}':\")\n",
    "print(f\"Retrieved content length: {len(test_result)} characters\")\n",
    "print(f\"Sample content: {test_result[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ Section 5: Build Decision Router Function\n",
    "\n",
    "This is the brain of our Agentic RAG system! The router decides whether a query can be answered using local documents or needs external web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Router Function\n",
    "def check_local_knowledge(query: str, context: str) -> bool:\n",
    "    \"\"\"\n",
    "    Router function to determine if we can answer from local knowledge\n",
    "    Returns True if local documents contain sufficient information\n",
    "    \"\"\"\n",
    "    \n",
    "    router_prompt = '''Role: Intelligent Query Router\n",
    "Task: Determine whether the local documents contain sufficient information to answer the user's question.\n",
    "\n",
    "Instructions:\n",
    "- Analyze the provided context and user question carefully\n",
    "- Respond with ONLY \"Yes\" or \"No\" \n",
    "- \"Yes\" if the local context contains relevant information to answer the question\n",
    "- \"No\" if external search is needed for complete information\n",
    "\n",
    "Examples:\n",
    "Question: \"What is machine learning?\"\n",
    "Context: \"Machine learning is a subset of artificial intelligence...\"\n",
    "Answer: Yes\n",
    "\n",
    "Question: \"What's the latest news about AI?\"\n",
    "Context: \"Machine learning is a subset of artificial intelligence...\"\n",
    "Answer: No\n",
    "\n",
    "Current Question: {query}\n",
    "Available Context: {context}\n",
    "\n",
    "Answer:'''\n",
    "\n",
    "    try:\n",
    "        formatted_prompt = router_prompt.format(query=query, context=context)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        decision = response.content.strip().lower()\n",
    "        return \"yes\" in decision\n",
    "    except Exception as e:\n",
    "        print(f\"Router error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the router function\n",
    "print(\"üß≠ Testing the decision router...\")\n",
    "\n",
    "# Get some local context for testing\n",
    "local_context = get_local_content(\"\", k=3)  # Get general context\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"What is Agentic RAG?\",\n",
    "    \"What are the latest AI news from 2024?\", \n",
    "    \"Explain machine learning types\",\n",
    "    \"What's the weather today?\"\n",
    "]\n",
    "\n",
    "for query in test_cases:\n",
    "    decision = check_local_knowledge(query, local_context)\n",
    "    route = \"LOCAL\" if decision else \"WEB\"\n",
    "    print(f\"   Query: '{query}' ‚Üí Route: {route}\")\n",
    "\n",
    "print(\"\\n‚úÖ Router function is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Section 6: Implement Web Search and Scraping Agent\n",
    "\n",
    "When local knowledge isn't sufficient, our CrewAI agents will search the web and scrape relevant content to provide up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search and Scraping Agents\n",
    "print(\"üåê Setting up web search and scraping agents...\")\n",
    "\n",
    "def setup_web_scraping_agent():\n",
    "    \"\"\"Setup the web scraping agent and related components\"\"\"\n",
    "    \n",
    "    # Initialize search and scraping tools\n",
    "    search_tool = SerperDevTool(api_key=SERPER_API_KEY)\n",
    "    scrape_tool = ScrapeWebsiteTool()\n",
    "    \n",
    "    # Define the web search agent\n",
    "    web_search_agent = Agent(\n",
    "        role=\"Expert Web Search Specialist\",\n",
    "        goal=\"Find the most relevant and up-to-date web sources for user queries\",\n",
    "        backstory=\"\"\"You are an expert at identifying valuable web sources and finding \n",
    "        the most relevant articles, papers, and resources for any given topic. You have \n",
    "        a keen eye for quality and relevance.\"\"\",\n",
    "        allow_delegation=False,\n",
    "        verbose=False,\n",
    "        llm=crew_llm\n",
    "    )\n",
    "    \n",
    "    # Define the web scraping agent  \n",
    "    web_scraper_agent = Agent(\n",
    "        role=\"Expert Content Analyzer and Summarizer\",\n",
    "        goal=\"Extract and analyze key information from web pages\",\n",
    "        backstory=\"\"\"You are highly skilled at analyzing web content, extracting key \n",
    "        insights, and summarizing complex information in a clear and concise manner. \n",
    "        You focus on the most relevant information for the user's needs.\"\"\",\n",
    "        allow_delegation=False,\n",
    "        verbose=False,\n",
    "        llm=crew_llm\n",
    "    )\n",
    "    \n",
    "    # Define the web search task\n",
    "    search_task = Task(\n",
    "        description=\"\"\"\n",
    "        Search for the most relevant and recent information about: '{topic}'.\n",
    "        Find authoritative sources, recent articles, or academic papers.\n",
    "        Provide the URL and a brief summary of why this source is valuable.\n",
    "        \"\"\",\n",
    "        expected_output=\"\"\"\n",
    "        The URL of the most relevant web source and a brief explanation of its relevance to '{topic}'.\n",
    "        Include key points that make this source valuable.\n",
    "        \"\"\",\n",
    "        tools=[search_tool],\n",
    "        agent=web_search_agent,\n",
    "    )\n",
    "    \n",
    "    # Define the web scraping task\n",
    "    scraping_task = Task(\n",
    "        description=\"\"\"\n",
    "        Extract and analyze the content from the web source found in the previous task.\n",
    "        Focus on information relevant to: '{topic}'.\n",
    "        Summarize the key findings in a clear and structured way.\n",
    "        \"\"\",\n",
    "        expected_output=\"\"\"\n",
    "        A comprehensive summary of the web content related to '{topic}', including:\n",
    "        - Key facts and insights\n",
    "        - Important details and explanations\n",
    "        - Relevant examples or case studies\n",
    "        Ensure the summary is accurate and well-organized.\n",
    "        \"\"\",\n",
    "        tools=[scrape_tool],\n",
    "        agent=web_scraper_agent,\n",
    "    )\n",
    "    \n",
    "    # Create the crew\n",
    "    crew = Crew(\n",
    "        agents=[web_search_agent, web_scraper_agent],\n",
    "        tasks=[search_task, scraping_task],\n",
    "        verbose=False,\n",
    "        memory=False,\n",
    "    )\n",
    "    \n",
    "    return crew\n",
    "\n",
    "def get_web_content(query: str) -> str:\n",
    "    \"\"\"Get content from web scraping\"\"\"\n",
    "    try:\n",
    "        print(f\"   üîç Searching web for: '{query}'\")\n",
    "        crew = setup_web_scraping_agent()\n",
    "        result = crew.kickoff(inputs={\"topic\": query})\n",
    "        return result.raw\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Web search error: {e}\")\n",
    "        return f\"Web search unavailable. Using fallback information about: {query}\"\n",
    "\n",
    "print(\"‚úÖ Web agents configured successfully!\")\n",
    "\n",
    "# Test the web search (commented out to avoid API calls during demo)\n",
    "# Uncomment the following lines to test web search functionality\n",
    "# test_web_query = \"latest AI developments 2024\"\n",
    "# web_result = get_web_content(test_web_query)\n",
    "# print(f\"üß™ Web search test completed for: '{test_web_query}'\")\n",
    "# print(f\"Result length: {len(web_result)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Section 7: Create Document Relevance Grader\n",
    "\n",
    "The grader evaluates whether retrieved documents are relevant to the user's question and decides the next step in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Relevance Grader\n",
    "print(\"‚öñÔ∏è Setting up document relevance grader...\")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Grade documents for relevance to user question\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, 'no' if not relevant\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Brief explanation for the grading decision\"\n",
    "    )\n",
    "\n",
    "def grade_documents(question: str, context: str) -> dict:\n",
    "    \"\"\"\n",
    "    Determine whether retrieved documents are relevant to the question\n",
    "    Returns: Dictionary with 'relevant' (bool) and 'reasoning' (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    grade_prompt = f\"\"\"\n",
    "    You are an expert document grader. Assess whether the retrieved document \n",
    "    is relevant to the user question.\n",
    "    \n",
    "    Retrieved Document: {context}\n",
    "    \n",
    "    User Question: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    - If the document contains keywords or semantic meaning related to the question, grade as 'yes'\n",
    "    - If the document is completely unrelated or doesn't help answer the question, grade as 'no'\n",
    "    - Provide brief reasoning for your decision\n",
    "    \n",
    "    Respond with your grading decision and reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create a simpler grading approach using regular LLM\n",
    "        response = llm.invoke(grade_prompt)\n",
    "        content = response.content.lower()\n",
    "        \n",
    "        # Simple parsing of response\n",
    "        if \"yes\" in content or \"relevant\" in content:\n",
    "            relevant = True\n",
    "        else:\n",
    "            relevant = False\n",
    "            \n",
    "        reasoning = response.content[:100] + \"...\" if len(response.content) > 100 else response.content\n",
    "        \n",
    "        return {\n",
    "            \"relevant\": relevant,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"next_action\": \"generate_answer\" if relevant else \"rewrite_question\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Grading error: {e}\")\n",
    "        return {\n",
    "            \"relevant\": True,  # Default to relevant on error\n",
    "            \"reasoning\": \"Error in grading, defaulting to relevant\",\n",
    "            \"next_action\": \"generate_answer\"\n",
    "        }\n",
    "\n",
    "# Test the grader\n",
    "print(\"üß™ Testing document grader...\")\n",
    "\n",
    "test_contexts = [\n",
    "    (\"What is machine learning?\", \"Machine learning is a subset of artificial intelligence that enables computers to learn...\"),\n",
    "    (\"What's the weather today?\", \"Machine learning is a subset of artificial intelligence that enables computers to learn...\"),\n",
    "    (\"Explain vector databases\", \"Vector databases are specialized databases designed to store and query high-dimensional vectors...\")\n",
    "]\n",
    "\n",
    "for question, context in test_contexts:\n",
    "    grade_result = grade_documents(question, context)\n",
    "    print(f\"   Q: '{question}' ‚Üí Relevant: {grade_result['relevant']} ‚Üí Action: {grade_result['next_action']}\")\n",
    "\n",
    "print(\"‚úÖ Document grader is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úèÔ∏è Section 8: Build Query Rewriter for Better Retrieval\n",
    "\n",
    "When initial retrieval returns irrelevant documents, the query rewriter improves the search terms for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Rewriter\n",
    "print(\"‚úèÔ∏è Setting up query rewriter...\")\n",
    "\n",
    "def rewrite_question(original_question: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite the original question to improve retrieval results\n",
    "    \"\"\"\n",
    "    \n",
    "    rewrite_prompt = f\"\"\"\n",
    "    You are a query optimization expert. Your task is to rewrite the user's question \n",
    "    to improve information retrieval.\n",
    "    \n",
    "    Original Question: {original_question}\n",
    "    \n",
    "    Instructions:\n",
    "    - Analyze the underlying intent of the question\n",
    "    - Rewrite to be more specific and searchable\n",
    "    - Include relevant keywords and context\n",
    "    - Make it clearer and more precise\n",
    "    - Keep the core meaning unchanged\n",
    "    \n",
    "    Provide only the rewritten question, nothing else.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(rewrite_prompt)\n",
    "        rewritten = response.content.strip()\n",
    "        return rewritten\n",
    "    except Exception as e:\n",
    "        print(f\"Query rewriting error: {e}\")\n",
    "        return original_question  # Return original on error\n",
    "\n",
    "# Test the query rewriter\n",
    "print(\"üß™ Testing query rewriter...\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is AI?\",\n",
    "    \"Tell me about ML\",\n",
    "    \"How does it work?\",\n",
    "    \"Explain the concept\",\n",
    "    \"What are the types?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    rewritten = rewrite_question(question)\n",
    "    print(f\"   Original: '{question}'\")\n",
    "    print(f\"   Rewritten: '{rewritten}'\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Query rewriter is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Section 9: Generate Final Answers with Context\n",
    "\n",
    "This function combines retrieved context with user queries to produce comprehensive, accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Answer Generation\n",
    "print(\"üí¨ Setting up answer generation...\")\n",
    "\n",
    "def generate_final_answer(context: str, query: str, source_type: str = \"unknown\") -> str:\n",
    "    \"\"\"\n",
    "    Generate final answer using retrieved context and user query\n",
    "    \"\"\"\n",
    "    \n",
    "    answer_prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Use the provided context to answer the user's question accurately and comprehensively.\n",
    "    \n",
    "    Context Information:\n",
    "    {context}\n",
    "    \n",
    "    User Question: {query}\n",
    "    \n",
    "    Instructions:\n",
    "    - Base your answer primarily on the provided context\n",
    "    - Be accurate and informative\n",
    "    - If the context doesn't fully answer the question, acknowledge this\n",
    "    - Provide a clear and well-structured response\n",
    "    - Include relevant details from the context\n",
    "    - Keep the answer focused and helpful\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(answer_prompt)\n",
    "        answer = response.content.strip()\n",
    "        \n",
    "        # Add source information\n",
    "        source_note = f\"\\n\\nüìö *Source: {source_type.title()} knowledge base*\"\n",
    "        return answer + source_note\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer generation error: {e}\")\n",
    "        return f\"I apologize, but I encountered an error while generating the answer. The query was: {query}\"\n",
    "\n",
    "# Test answer generation\n",
    "print(\"üß™ Testing answer generation...\")\n",
    "\n",
    "test_context = \"\"\"\n",
    "Agentic RAG represents a significant evolution in AI systems, combining the decision-making \n",
    "capabilities of AI agents with traditional RAG approaches. Unlike standard RAG systems that \n",
    "simply retrieve and generate, Agentic RAG systems can make intelligent routing decisions, \n",
    "evaluate retrieval quality, rewrite queries for better results, and combine multiple information sources.\n",
    "\"\"\"\n",
    "\n",
    "test_query = \"What makes Agentic RAG different from traditional RAG?\"\n",
    "test_answer = generate_final_answer(test_context, test_query, \"local\")\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Answer: {test_answer}\")\n",
    "\n",
    "print(\"\\n‚úÖ Answer generation is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Section 10: Assemble the Complete Agentic RAG Pipeline\n",
    "\n",
    "Now we'll integrate all components into a cohesive pipeline with proper routing logic and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Agentic RAG Pipeline\n",
    "print(\"üîó Assembling the complete Agentic RAG pipeline...\")\n",
    "\n",
    "class AgenticRAGPipeline:\n",
    "    \"\"\"\n",
    "    Complete Agentic RAG Pipeline that intelligently routes queries\n",
    "    between local documents and web search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db, max_retries=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "    def process_query(self, query: str, verbose: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Main pipeline function to process user queries\n",
    "        \n",
    "        Returns:\n",
    "            dict: Contains answer, source_type, processing_steps, and metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        processing_steps = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüîç Processing query: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Get initial context for routing decision\n",
    "            processing_steps.append(\"Getting local context for routing\")\n",
    "            if verbose:\n",
    "                print(\"   üìö Getting local context for routing...\")\n",
    "            \n",
    "            local_context = get_local_content(\"general knowledge\", k=3)\n",
    "            \n",
    "            # Step 2: Route the query\n",
    "            processing_steps.append(\"Making routing decision\")\n",
    "            if verbose:\n",
    "                print(\"   üß≠ Making routing decision...\")\n",
    "            \n",
    "            can_answer_locally = check_local_knowledge(query, local_context)\n",
    "            route = \"LOCAL\" if can_answer_locally else \"WEB\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   üéØ Routing decision: {route}\")\n",
    "            \n",
    "            # Step 3: Retrieve context based on routing decision\n",
    "            if can_answer_locally:\n",
    "                # Local retrieval path\n",
    "                processing_steps.append(\"Retrieving from local documents\")\n",
    "                if verbose:\n",
    "                    print(\"   üìñ Retrieving from local documents...\")\n",
    "                \n",
    "                context = get_local_content(query, k=5)\n",
    "                source_type = \"local\"\n",
    "                \n",
    "                # Step 4: Grade the retrieved documents\n",
    "                processing_steps.append(\"Grading document relevance\")\n",
    "                if verbose:\n",
    "                    print(\"   ‚öñÔ∏è Grading document relevance...\")\n",
    "                \n",
    "                grade_result = grade_documents(query, context)\n",
    "                \n",
    "                # Step 5: Handle grading result\n",
    "                if not grade_result[\"relevant\"]:\n",
    "                    processing_steps.append(\"Documents not relevant, rewriting query\")\n",
    "                    if verbose:\n",
    "                        print(\"   ‚úèÔ∏è Documents not relevant, rewriting query...\")\n",
    "                    \n",
    "                    # Rewrite and try again (limited retries)\n",
    "                    rewritten_query = rewrite_question(query)\n",
    "                    if verbose:\n",
    "                        print(f\"   üîÑ Rewritten query: '{rewritten_query}'\")\n",
    "                    \n",
    "                    context = get_local_content(rewritten_query, k=5)\n",
    "                    \n",
    "            else:\n",
    "                # Web search path\n",
    "                processing_steps.append(\"Searching web for external information\")\n",
    "                if verbose:\n",
    "                    print(\"   üåê Searching web for external information...\")\n",
    "                \n",
    "                context = get_web_content(query)\n",
    "                source_type = \"web\"\n",
    "            \n",
    "            # Step 6: Generate final answer\n",
    "            processing_steps.append(\"Generating final answer\")\n",
    "            if verbose:\n",
    "                print(\"   üí¨ Generating final answer...\")\n",
    "            \n",
    "            answer = generate_final_answer(context, query, source_type)\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Return comprehensive result\n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"source_type\": source_type,\n",
    "                \"processing_steps\": processing_steps,\n",
    "                \"processing_time\": f\"{processing_time:.2f}s\",\n",
    "                \"context_length\": len(context),\n",
    "                \"route_decision\": route,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   ‚úÖ Processing completed in {processing_time:.2f}s\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_result = {\n",
    "                \"query\": query,\n",
    "                \"answer\": f\"I apologize, but I encountered an error while processing your query: {str(e)}\",\n",
    "                \"source_type\": \"error\",\n",
    "                \"processing_steps\": processing_steps + [f\"Error: {str(e)}\"],\n",
    "                \"processing_time\": f\"{time.time() - start_time:.2f}s\",\n",
    "                \"success\": False\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "            \n",
    "            return error_result\n",
    "\n",
    "# Initialize the pipeline\n",
    "agentic_rag = AgenticRAGPipeline(vector_db)\n",
    "\n",
    "print(\"‚úÖ Agentic RAG Pipeline assembled successfully!\")\n",
    "print(\"üöÄ Ready to process queries intelligently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Section 11: Demo - Interactive Query Processing\n",
    "\n",
    "Let's test our Agentic RAG system with various types of queries to see how it intelligently routes and processes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Demo - Test Various Query Types\n",
    "print(\"üéÆ Starting Interactive Agentic RAG Demo!\\n\")\n",
    "\n",
    "# Define test queries that should demonstrate different routing behaviors\n",
    "demo_queries = [\n",
    "    {\n",
    "        \"query\": \"What is Agentic RAG and how does it work?\",\n",
    "        \"expected_route\": \"LOCAL\",\n",
    "        \"description\": \"Query about content in our local documents\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the different types of machine learning\",\n",
    "        \"expected_route\": \"LOCAL\", \n",
    "        \"description\": \"Another local knowledge query\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are vector databases used for?\",\n",
    "        \"expected_route\": \"LOCAL\",\n",
    "        \"description\": \"Technical query covered in our documents\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the latest AI news and developments in 2024?\",\n",
    "        \"expected_route\": \"WEB\",\n",
    "        \"description\": \"Current events requiring web search\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the weather forecast for tomorrow?\",\n",
    "        \"expected_route\": \"WEB\", \n",
    "        \"description\": \"Real-time information not in local docs\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Helper function to display results nicely\n",
    "def display_result(result: dict, query_info: dict):\n",
    "    \"\"\"Display query results in a formatted way\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üîç QUERY: {result['query']}\")\n",
    "    print(f\"üìù Description: {query_info['description']}\")\n",
    "    print(f\"üéØ Expected Route: {query_info['expected_route']} | Actual Route: {result['route_decision']}\")\n",
    "    print(f\"üìä Source: {result['source_type'].upper()} | Time: {result['processing_time']}\")\n",
    "    print(f\"üìè Context Length: {result['context_length']} characters\")\n",
    "    print(\"\\nüìã Processing Steps:\")\n",
    "    for i, step in enumerate(result['processing_steps'], 1):\n",
    "        print(f\"   {i}. {step}\")\n",
    "    print(f\"\\nüí¨ ANSWER:\")\n",
    "    print(result['answer'])\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "\n",
    "# Run the demo\n",
    "print(\"üöÄ Running Agentic RAG Demo with different query types...\\n\")\n",
    "\n",
    "for i, query_info in enumerate(demo_queries, 1):\n",
    "    print(f\"üìå Demo Query #{i}\")\n",
    "    \n",
    "    # Process the query\n",
    "    result = agentic_rag.process_query(query_info[\"query\"], verbose=False)\n",
    "    \n",
    "    # Display results\n",
    "    display_result(result, query_info)\n",
    "    \n",
    "    # Add a small delay between queries for readability\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"üéâ Demo completed! The Agentic RAG system successfully:\")\n",
    "print(\"   ‚úÖ Routed queries to appropriate sources\")  \n",
    "print(\"   ‚úÖ Retrieved relevant information\")\n",
    "print(\"   ‚úÖ Generated comprehensive answers\")\n",
    "print(\"   ‚úÖ Provided detailed processing insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Section 12: Compare Responses - Local vs Web-Enhanced\n",
    "\n",
    "Let's run side-by-side comparisons to see how the system handles the same query types with different routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Analysis: Local vs Web Responses\n",
    "print(\"üîÑ Comparison Analysis: Local vs Web Enhanced Responses\\n\")\n",
    "\n",
    "def compare_responses(query: str):\n",
    "    \"\"\"\n",
    "    Force both local and web responses for the same query to compare approaches\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyzing Query: '{query}'\\n\")\n",
    "    \n",
    "    # Force local response\n",
    "    print(\"üìö LOCAL RESPONSE:\")\n",
    "    print(\"-\" * 50)\n",
    "    local_context = get_local_content(query, k=5)\n",
    "    local_answer = generate_final_answer(local_context, query, \"local\")\n",
    "    print(f\"Context Length: {len(local_context)} characters\")\n",
    "    print(f\"Answer: {local_answer}\\n\")\n",
    "    \n",
    "    # Force web response (simulate)\n",
    "    print(\"üåê WEB-ENHANCED RESPONSE:\")\n",
    "    print(\"-\" * 50)\n",
    "    # For demo purposes, we'll simulate web content since we don't want to make actual API calls\n",
    "    web_context = f\"\"\"\n",
    "    [Simulated Web Content for: {query}]\n",
    "    This would contain up-to-date information from web search about {query.lower()}, \n",
    "    including recent developments, current statistics, and latest research findings \n",
    "    that might not be available in the local document database.\n",
    "    \"\"\"\n",
    "    web_answer = generate_final_answer(web_context, query, \"web\")\n",
    "    print(f\"Context Length: {len(web_context)} characters\")\n",
    "    print(f\"Answer: {web_answer}\\n\")\n",
    "    \n",
    "    print(\"üìä COMPARISON SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"‚Ä¢ Local response length: {len(local_answer)} characters\")\n",
    "    print(f\"‚Ä¢ Web response length: {len(web_answer)} characters\") \n",
    "    print(f\"‚Ä¢ Local context length: {len(local_context)} characters\")\n",
    "    print(f\"‚Ä¢ Web context length: {len(web_context)} characters\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test queries for comparison\n",
    "comparison_queries = [\n",
    "    \"What is Agentic RAG?\",\n",
    "    \"Explain machine learning fundamentals\", \n",
    "    \"How do vector databases work?\"\n",
    "]\n",
    "\n",
    "print(\"Running side-by-side comparisons...\\n\")\n",
    "\n",
    "for query in comparison_queries:\n",
    "    compare_responses(query)\n",
    "\n",
    "print(\"üéØ KEY INSIGHTS from the comparison:\")\n",
    "print(\"\"\"\n",
    "1. üìö LOCAL RESPONSES:\n",
    "   ‚Ä¢ Faster processing (no web API calls)\n",
    "   ‚Ä¢ Consistent with our curated knowledge base\n",
    "   ‚Ä¢ Limited to information available in local documents\n",
    "   ‚Ä¢ Ideal for domain-specific knowledge\n",
    "\n",
    "2. üåê WEB-ENHANCED RESPONSES:  \n",
    "   ‚Ä¢ Access to current and broader information\n",
    "   ‚Ä¢ Takes longer due to search and scraping\n",
    "   ‚Ä¢ Can provide more recent developments\n",
    "   ‚Ä¢ Better for current events and expanding knowledge\n",
    "\n",
    "3. üß† AGENTIC ROUTING BENEFITS:\n",
    "   ‚Ä¢ Automatically chooses the best source\n",
    "   ‚Ä¢ Optimizes for both speed and completeness\n",
    "   ‚Ä¢ Reduces unnecessary web searches\n",
    "   ‚Ä¢ Provides consistent user experience\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusion and Next Steps\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "Congratulations! You've successfully built a complete **Agentic RAG system** that:\n",
    "\n",
    "‚úÖ **Intelligently Routes Queries** - Decides between local and web sources  \n",
    "‚úÖ **Processes Local Documents** - Chunks, embeds, and searches vector database  \n",
    "‚úÖ **Web Search & Scraping** - Uses AI agents for external information gathering  \n",
    "‚úÖ **Grades Document Relevance** - Evaluates retrieval quality  \n",
    "‚úÖ **Rewrites Queries** - Optimizes search terms for better results  \n",
    "‚úÖ **Generates Contextual Answers** - Combines information sources effectively  \n",
    "\n",
    "### Key Components Recap\n",
    "\n",
    "| Component | Purpose | Technology |\n",
    "|-----------|---------|------------|\n",
    "| üß≠ **Router** | Decides information source | Groq LLM + Prompt Engineering |\n",
    "| üìö **Vector DB** | Local document search | FAISS + HuggingFace Embeddings |\n",
    "| üåê **Web Agents** | External information gathering | CrewAI + Serper + Web Scraping |\n",
    "| ‚öñÔ∏è **Grader** | Relevance evaluation | LLM-based scoring |\n",
    "| ‚úèÔ∏è **Rewriter** | Query optimization | LLM-based rewriting |\n",
    "| üí¨ **Generator** | Final answer synthesis | Context-aware generation |\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "üöÄ **Performance**: Fast local retrieval when possible  \n",
    "üéØ **Accuracy**: Web search for up-to-date information  \n",
    "üß† **Intelligence**: Automatic routing and optimization  \n",
    "üîÑ **Adaptability**: Self-improving through query rewriting  \n",
    "üìà **Scalability**: Easy to add new data sources  \n",
    "\n",
    "### Next Steps & Extensions\n",
    "\n",
    "1. **üìä Add Analytics**: Track routing decisions and performance metrics\n",
    "2. **üîß Optimize Embeddings**: Fine-tune for your specific domain\n",
    "3. **üåç Multi-Language Support**: Extend to other languages\n",
    "4. **üì± Build UI**: Create a web interface for user interaction\n",
    "5. **üîó Database Integration**: Connect to live databases\n",
    "6. **üß™ A/B Testing**: Compare different routing strategies\n",
    "7. **üìà Monitoring**: Add logging and performance tracking\n",
    "8. **üîí Security**: Implement authentication and rate limiting\n",
    "\n",
    "### Try It Yourself!\n",
    "\n",
    "Use the interactive demo above to test with your own queries, or modify the code to:\n",
    "- Add your own PDF documents\n",
    "- Experiment with different LLM models\n",
    "- Adjust routing logic and prompts\n",
    "- Integrate additional data sources\n",
    "\n",
    "**Happy building with Agentic RAG!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéÆ Try Your Own Query!\n",
    "# Uncomment and modify the code below to test with your own questions\n",
    "\n",
    "\"\"\"\n",
    "# Example: Test with your own query\n",
    "my_query = \"Your question here\"\n",
    "\n",
    "# Process the query\n",
    "result = agentic_rag.process_query(my_query, verbose=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üîç YOUR QUERY: {result['query']}\")\n",
    "print(f\"üéØ Route Decision: {result['route_decision']}\")\n",
    "print(f\"üìä Source: {result['source_type'].upper()}\")\n",
    "print(f\"‚è±Ô∏è Processing Time: {result['processing_time']}\")\n",
    "print(f\"\\nüí¨ ANSWER:\")\n",
    "print(result['answer'])\n",
    "print(\"=\"*80)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üéâ Agentic RAG Demo Complete!\")\n",
    "print(\"üìù Edit the cell above to test your own queries\")\n",
    "print(\"üöÄ The system is ready for your experiments!\")\n",
    "\n",
    "# Optional: Create a simple interactive loop (uncomment to use)\n",
    "\"\"\"\n",
    "def interactive_demo():\n",
    "    print(\"üéÆ Interactive Agentic RAG Demo\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"üí¨ Ask me anything: \")\n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        result = agentic_rag.process_query(query, verbose=False)\n",
    "        print(f\"\\nü§ñ Answer ({result['source_type']} source):\")\n",
    "        print(result['answer'])\n",
    "        print(f\"\\n‚è±Ô∏è Processed in {result['processing_time']}\")\n",
    "        print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# interactive_demo()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}